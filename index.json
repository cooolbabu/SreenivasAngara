[{"content":"Non-linear regression is a powerful technique used in statistics and machine learning to model and analyze complex relationships between variables. Unlike linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression algorithms can capture more intricate and non-linear patterns. This makes them particularly useful when dealing with real-world phenomena where linear relationships are insufficient.\nFollowing Non-Linear regression models are covered in the attached Jypter notebook\nDecision Tree Regressor: A decision tree regressor is a non-linear regression algorithm that creates a hierarchical structure of binary decisions to predict the target variable. It splits the data based on feature values and predicts the average value of the target variable within each leaf node. Decision trees are easy to interpret but can suffer from overfitting. Bagging Regressor: The Bagging Regressor is an ensemble learning algorithm that combines multiple decision tree regressors. It trains each decision tree on a random subset of the training data, allowing them to learn different patterns. The final prediction is the average of the predictions made by each tree. Bagging helps reduce overfitting and improves model stability. Random Forest Regressor: Random Forest is another ensemble algorithm that builds a collection of decision trees. It introduces randomness by selecting a subset of features at each split and training each tree independently. Random Forests are effective in capturing complex non-linear relationships and provide robust predictions by aggregating the outputs of multiple trees. Ada Boost Regressor: Ada Boost (Adaptive Boosting) is an ensemble algorithm that combines weak learners into a strong learner. In the context of regression, weak learners are decision trees with limited depth. Ada Boost assigns higher weights to data points that were incorrectly predicted by previous models, allowing subsequent models to focus on those points. It iteratively improves predictions by adjusting the weights of the weak learners. Gradient Boosting Regressor: Gradient Boosting Regressor is another ensemble algorithm that combines weak learners, typically decision trees, in a sequential manner. It fits each subsequent tree to the residual errors of the previous tree. By minimizing the errors in a gradient descent-like manner, Gradient Boosting produces a strong predictive model. It is particularly effective in handling complex non-linear relationships.\nXG Boost Regressor: XG Boost (Extreme Gradient Boosting) is an optimized implementation of the Gradient Boosting algorithm. It uses a more efficient and accurate tree-building process, handling missing values, regularization techniques, and parallel processing. XG Boost has gained popularity due to its high performance and effectiveness in various regression problems. These regression models offer different trade-offs in terms of interpretability, accuracy, and computational complexity. Choosing the most suitable model depends on the specific problem and dataset characteristics. It is often beneficial to experiment and compare multiple models to find the best fit for the task at hand.\nPerformance Report The highlighted model is a tuned Random Forest Regressor using GridSearch. We can see that both R-Squre and Adjusted R-Square are higher and error scores are lower than models that are not tuned. Tuning the hyperparameters of a machine learning model can help improve its performance.\nFeature Importance We can extract important features and visualize them. Here\nThe most important features are Department_gynecology, Age_41_50, and Age_31_40, followed by Department_anesthesia, Department_radiotherapy, and Admission_Deposit. The rest of the variables have little or no influence on the length of stay in the hospital in this model. ðŸ”— Jupyter Notebook ðŸ”— HTML version ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/non-linearregression/","summary":"Non-linear regression is a powerful technique used in statistics and machine learning to model and analyze complex relationships between variables. Unlike linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression algorithms can capture more intricate and non-linear patterns. This makes them particularly useful when dealing with real-world phenomena where linear relationships are insufficient.\nFollowing Non-Linear regression models are covered in the attached Jypter notebook","title":"Non-Linear Regression models"},{"content":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.\nTrue being - the Loan will default False - the loan will not default An essential aspect of being a successful Data Scientist lies in effectively presenting findings. Seaborn, a remarkable visualization tool, played a pivotal role in crafting captivating presentations.\nThe following three charts encapsulate the efforts made during the Capstone project.\nPresentation.1 - Performance of various ML models for Loan default classification The following table shows the preformance score for each model. F1 Score is measure of models accuracy\nPresentation.2 - Selecting Top three classification Models for predicting loan default Every model has a Train and Test score. We would like to see these scores as close as possible. Presentation.3 - Selecting the best classification model for loan default Here both Stacking Classification and HistGradientBoosting Classifier have simillar or closer F1 Scores. Digging deeper we can see that HistGradientBoosting is better at predicting actual defaults. And that\u0026rsquo;s the winner\nðŸ”— Jupyter Notebook ðŸ”— HTML version ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/mit_capstone/","summary":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.","title":"MIT-GreatLearning Applied Data Science program Capstone Project"},{"content":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.\nGenerative AI is a disruptive technology that evokes both awe and apprehension. It has the potential to revolutionize numerous professions, and as a Machine Learning engineer, I was curious to explore its capabilities. Currently, I am enrolled in an \u0026ldquo;Applied Data Science and Machine Learning\u0026rdquo; bootcamp, aiming to discover the impact of Generative AI firsthand. It took me approximately six months to acquire proficiency in Python programming and Machine Learning techniques. To challenge myself further, I sought to explore a topic completely unfamiliar to me, something that hadn\u0026rsquo;t been covered in the course and hadn\u0026rsquo;t been searched on Google yet. Through an NPR segment, I came across Sentiment Analysis.\nDuring my exploration of Generative AI, I put it to the test by asking ChatGPT just two questions. Astonishingly, within a mere two minutes, it provided me with an answer. This experience left me truly amazed.\nQuestion 1 You are a data scientist performing Sentiment analysis. I have a customer satisfaction problem. I want to know, why customers are dissatisfied with my products in the Computers category. You need to read comments and tell me if .. 1) Product is high priced 2) Issue with product quality 3) unhappy with delivery. What deep learning algorithm should I use to solve this problem\nFor the customer satisfaction problem in the Computers category, where you want to identify the reasons for customer dissatisfaction based on comments, sentiment analysis can be an effective approach. Specifically, you can use a deep learning algorithm called a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells.\nLSTM-based RNNs are well-suited for analyzing sequential data, such as sentences or comments, because they can capture the dependencies and long-term relationships between words. This makes them effective for sentiment analysis tasks where the order of words and their context is important.\nQuestion 2 Let\u0026rsquo;s use Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells. The data is in a file that\u0026rsquo;s in Parquet format. It has customer_id, product_id and comments. Give me python machine learning code that implement this alogorithm: 1) That finds the best parameters to use and then use the best parameters to build a model that I can use in my production environment\nCertainly! To implement the RNN with LSTM cells for sentiment analysis using customer comments, and to find the best parameters for your model, you can follow the code example provided below. This example assumes you have the necessary libraries such as pandas, scikit-learn, TensorFlow, and Keras installed.\nimport pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense # Load the data from the Parquet file df = pd.read_parquet(\u0026quot;data.parquet\u0026quot;) # Split the data into input features (comments) and target variable (reason for dissatisfaction) X = df[\u0026quot;comments\u0026quot;].values y = df[\u0026quot;dissatisfaction_reason\u0026quot;].values # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Tokenize the input text and convert it to sequences tokenizer = Tokenizer() tokenizer.fit_on_texts(X_train) X_train_seq = tokenizer.texts_to_sequences(X_train) X_test_seq = tokenizer.texts_to_sequences(X_test) # Pad the sequences to ensure they have the same length max_sequence_length = max([len(seq) for seq in X_train_seq]) X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length) X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length) # Define the LSTM model model = Sequential() model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(units=3, activation='softmax')) # Compile the model model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Define the parameter grid for grid search param_grid = { 'units': [64, 128, 256], 'dropout': [0.2, 0.3, 0.4], 'recurrent_dropout': [0.2, 0.3, 0.4] } # Perform grid search to find the best parameters grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3) grid_search.fit(X_train_padded, y_train) # Get the best parameters best_params = grid_search.best_params_ print(\u0026quot;Best Parameters:\u0026quot;, best_params) # Build the final model using the best parameters final_model = Sequential() final_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) final_model.add(LSTM(units=best_params['units'], dropout=best_params['dropout'], recurrent_dropout=best_params['recurrent_dropout'])) final_model.add(Dense(units=3, activation='softmax')) final_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Train the final model using the best parameters final_model.fit(X_train_padded, y_train, epochs=10, batch_size=32) # Evaluate the final model on the test set y_pred = final_model.predict_classes(X_test_padded) accuracy = accuracy_score(y_test, y_pred) print(\u0026quot;Accuracy:\u0026quot;, accuracy) # Save the final model for use in the product environment final_model.save(\u0026quot;sentiment_analysis_model.h5\u0026quot;) Final Thoughts Truly amazing! Prior to ChatGPT, we used to rely on code snippets that we copied and customized for our specific implementation. However, now we have the ability to obtain pre-customized programs that are ready to run. The next step is to construct a pipeline and execute the code.\nTaking it a step further, software developers can piece together individual components like puzzles and create a comprehensive final solution.\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/second/","summary":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.","title":"Conversations with ChatGPT - Machine Learning"},{"content":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all\nSource : Katrin Weitek Richard van der Blom\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/linkedin-visualization/","summary":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all","title":"Prepare yourself for LinkedIn"},{"content":"During an interview process, interviewers generally ask and expect textbook definitions of basic terms. Interviewees, on the other hand must be prepared and answer them promptly. Lengthy and exploratory answers prevent interviewers from asking more probing questions.\nConfidence Interval A confidence interval is a range of values around the point estimate, constructed so that this range will contain the population parameter with a certain degree of confidence, expressed in percentage terms. Used for testing null hypothesis\nVariance To calculate the variance, we take the difference between each number in the dataset and the mean of the data, square this difference to make it positive (independent of sign), and finally divide the sum of the squares by the total number of values in the dataset.\nBias Bias is the amount by which a modelâ€™s predictions differ from the target values, on the training data, i.e., the training error\nBias-Variance Trade-off If we increase the bias of an overfit model, we are making the model simpler and capable of generalizing over the validation set. Its performance on the validation set will improve and, consequently, the variance will decrease. On the other hand, if we decrease the bias of an underfit model, we are making the model more complex and the model will fit the training data more closely and, consequently, the variance will increase. This phenomenon is called the Bias-Variance tradeoff.\nHomoscedasticity - If the residuals are symmetrically distributed across the regression line, then the data is said to be homoscedastic.\nHeteroscedasticity - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case, the residuals can form a funnel shape or any other non-symmetrical shape\nQ_Q plot: The plot obtained is known as a quantile-quantile plot, or Q_Q plot, when the quantiles of two variables are placed against one another. With respect to the locations, this plot summarises whether or not the distributions of two variables are similar.\nPearsonr: A linear relationship between two variables\u0026rsquo; strength and direction is measured by the Pearson correlation coefficient. Values usually fall between -1 and 1, with 1 denoting a perfectly positive correlation and -1 denoting a perfectly inverse relationship.\nMulticolinearity - occurs when there is a dependency between independent variables\nVIF Variance inflation Factor. Review Celestial Object detection for details. 1 very less Multicollinerity \u0026lt; 5 Moderat VIF \u0026gt; 5 Extreme Multicollinearity Solutions Principle Component Analysis Drop some predictors Do some linear transformation e.g., add/subtract 2 predictors to create a new bespoke predictor Linear Regression Model performance R-squared: R-squared is a useful performance metric to understand how well the regression model has fitted over the training data. For example, an R-squared of 80% reveals that the model is able to capture 80% of the variation in the dependent variable. A higher R-squared value indicates a better fit for the model. Adjusted R-squared: The adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables present in the model. The R-squared always increases when a new independent variable is added to the model, irrespective of whether that variable adds value to the model or not. Hence, R-squared might be misleading when we have multiple independent variables and cannot identify unnecessary variables included in the model. RMSE: RMSE stands for Root Mean Squared Error. It is calculated as the square root of the mean of the squared differences between the actual output and the predictions. The lower the RMSE the better the performance of the model. Mathematically, it can be given as follows: However, when a new variable is added, the adjusted R-squared increases if that variable adds value to the model, and decreases if it does not. Hence, adjusted R-squared is a better choice of metric than R-squared to evaluate the quality of a regression model with multiple independent variables, because adjusted R-squared only remains high when all those independent variables are required to predict the value of the dependent variable well; it decreases if there are any independent variables which don\u0026rsquo;t have a significant effect on the predicted variable. MAE: The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average. MAPE: The mean absolute percentage error (MAPE) measures the accuracy of predictions as a percentage, and can be calculated as the average of absolute percentage error for all data points. The absolute percentage error is defined as predicted value minus actual values divided by actual values. It works best if there are no extreme values in the data and none of the actual values are 0. Classification model performance Accuracy - Overall Accuracy of the model. Precision - Use Precision to minimize False Negatives - Ideal for maketing campaigns Recall - Use Recall to minimize False Positives - Telling someone he/she has \u0026lsquo;Cancer\u0026rsquo; when not true F1-Score - Geometric mean of Precision and Recall. Overall Model performance Area under curve - Coverage of accurate predictions. ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/ml_key_terms/","summary":"During an interview process, interviewers generally ask and expect textbook definitions of basic terms. Interviewees, on the other hand must be prepared and answer them promptly. Lengthy and exploratory answers prevent interviewers from asking more probing questions.\nConfidence Interval A confidence interval is a range of values around the point estimate, constructed so that this range will contain the population parameter with a certain degree of confidence, expressed in percentage terms.","title":"Machnine Language Terms - Cheat sheet"},{"content":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/azurelandscape/","summary":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation","title":"Azure Landscape"},{"content":"WIP - Putting my thoughts together a future article on Azure API Management. Policies Authentication Policies Access Restriction Policy Caching Policy Cross Domain Policies Transformation Policies Advanced Policies\nMicroservice Microservices are fine grained loosely coupled services communicating through light-weight protocols. Azure functions versus Docker. Portability versus serverless\nAPI Management API Mangagement is a process of creating and publishing web application programming interfaces. Enforce usage policies, control access and analyse usage statistics\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/api_management/","summary":"WIP - Putting my thoughts together a future article on Azure API Management. Policies Authentication Policies Access Restriction Policy Caching Policy Cross Domain Policies Transformation Policies Advanced Policies\nMicroservice Microservices are fine grained loosely coupled services communicating through light-weight protocols. Azure functions versus Docker. Portability versus serverless\nAPI Management API Mangagement is a process of creating and publishing web application programming interfaces. Enforce usage policies, control access and analyse usage statistics","title":"WIP - API Management Concepts"},{"content":"WIP - A future article on Enterprise Application Platforms Platforms: JBoss, Microsoft Entity Framework Core, SpringBoot\nApplications: Web apps, REST services, SOA, JMS messaging\nLayers: Presentation layer, Buiness Layer, Database Layer (use mermaid.js for text to Illustration)\nKey concepts: Dependency Injection: Promotes loose coupling .. examples ??? Aspect Orient Programming: Apply application wide services to an application in a declarative fashion Logging, Transactions, Security POJO - Plain Old java object\nData Access Layer\nORM, JPA, Hibernate, JMS Build tools:\nMaven \u0026ndash; Personal shopper to get all your jar files \u0026ndash; Standard directory structure \u0026ndash; pom.xml ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/appcontainers/","summary":"WIP - A future article on Enterprise Application Platforms Platforms: JBoss, Microsoft Entity Framework Core, SpringBoot\nApplications: Web apps, REST services, SOA, JMS messaging\nLayers: Presentation layer, Buiness Layer, Database Layer (use mermaid.js for text to Illustration)\nKey concepts: Dependency Injection: Promotes loose coupling .. examples ??? Aspect Orient Programming: Apply application wide services to an application in a declarative fashion Logging, Transactions, Security POJO - Plain Old java object","title":"WIP - Application Platforms"},{"content":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/first/","summary":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","title":"Scratchpad"},{"content":"ðŸ”— Link to Resume ðŸ”— Credly â‡“â‡“ PDF version may take a few seconds to download. It will displayed here after download â‡“â‡“ ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/experience/resume/","summary":"ðŸ”— Link to Resume ðŸ”— Credly â‡“â‡“ PDF version may take a few seconds to download. It will displayed here after download â‡“â‡“ ","title":"Sreenivas Angara Resume"}]
[{"content":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.\nGenerative AI is a disruptive technology that evokes both awe and apprehension. It has the potential to revolutionize numerous professions, and as a Machine Learning engineer, I was curious to explore its capabilities. Currently, I am enrolled in an \u0026ldquo;Applied Data Science and Machine Learning\u0026rdquo; bootcamp, aiming to discover the impact of Generative AI firsthand. It took me approximately six months to acquire proficiency in Python programming and Machine Learning techniques. To challenge myself further, I sought to explore a topic completely unfamiliar to me, something that hadn\u0026rsquo;t been covered in the course and hadn\u0026rsquo;t been searched on Google yet. Through an NPR segment, I came across Sentiment Analysis.\nDuring my exploration of Generative AI, I put it to the test by asking ChatGPT just two questions. Astonishingly, within a mere two minutes, it provided me with an answer. This experience left me truly amazed.\nQuestion 1 You are a data scientist performing Sentiment analysis. I have a customer satisfaction problem. I want to know, why customers are dissatisfied with my products in the Computers category. You need to read comments and tell me if .. 1) Product is high priced 2) Issue with product quality 3) unhappy with delivery. What deep learning algorithm should I use to solve this problem\nFor the customer satisfaction problem in the Computers category, where you want to identify the reasons for customer dissatisfaction based on comments, sentiment analysis can be an effective approach. Specifically, you can use a deep learning algorithm called a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells.\nLSTM-based RNNs are well-suited for analyzing sequential data, such as sentences or comments, because they can capture the dependencies and long-term relationships between words. This makes them effective for sentiment analysis tasks where the order of words and their context is important.\nQuestion 2 Let\u0026rsquo;s use Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells. The data is in a file that\u0026rsquo;s in Parquet format. It has customer_id, product_id and comments. Give me python machine learning code that implement this alogorithm: 1) That finds the best parameters to use and then use the best parameters to build a model that I can use in my production environment\nCertainly! To implement the RNN with LSTM cells for sentiment analysis using customer comments, and to find the best parameters for your model, you can follow the code example provided below. This example assumes you have the necessary libraries such as pandas, scikit-learn, TensorFlow, and Keras installed.\nimport pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense # Load the data from the Parquet file df = pd.read_parquet(\u0026quot;data.parquet\u0026quot;) # Split the data into input features (comments) and target variable (reason for dissatisfaction) X = df[\u0026quot;comments\u0026quot;].values y = df[\u0026quot;dissatisfaction_reason\u0026quot;].values # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Tokenize the input text and convert it to sequences tokenizer = Tokenizer() tokenizer.fit_on_texts(X_train) X_train_seq = tokenizer.texts_to_sequences(X_train) X_test_seq = tokenizer.texts_to_sequences(X_test) # Pad the sequences to ensure they have the same length max_sequence_length = max([len(seq) for seq in X_train_seq]) X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length) X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length) # Define the LSTM model model = Sequential() model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(units=3, activation='softmax')) # Compile the model model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Define the parameter grid for grid search param_grid = { 'units': [64, 128, 256], 'dropout': [0.2, 0.3, 0.4], 'recurrent_dropout': [0.2, 0.3, 0.4] } # Perform grid search to find the best parameters grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3) grid_search.fit(X_train_padded, y_train) # Get the best parameters best_params = grid_search.best_params_ print(\u0026quot;Best Parameters:\u0026quot;, best_params) # Build the final model using the best parameters final_model = Sequential() final_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) final_model.add(LSTM(units=best_params['units'], dropout=best_params['dropout'], recurrent_dropout=best_params['recurrent_dropout'])) final_model.add(Dense(units=3, activation='softmax')) final_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Train the final model using the best parameters final_model.fit(X_train_padded, y_train, epochs=10, batch_size=32) # Evaluate the final model on the test set y_pred = final_model.predict_classes(X_test_padded) accuracy = accuracy_score(y_test, y_pred) print(\u0026quot;Accuracy:\u0026quot;, accuracy) # Save the final model for use in the product environment final_model.save(\u0026quot;sentiment_analysis_model.h5\u0026quot;) Final Thoughts Truly amazing! Prior to ChatGPT, we used to rely on code snippets that we copied and customized for our specific implementation. However, now we have the ability to obtain pre-customized programs that are ready to run. The next step is to construct a pipeline and execute the code.\nTaking it a step further, software developers can piece together individual components like puzzles and create a comprehensive final solution.\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/second/","summary":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.","title":"Conversations with ChatGPT - Machine Learning"},{"content":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.\nTrue being - the Loan will default False - the loan will not default An essential aspect of being a successful Data Scientist lies in effectively presenting findings. Seaborn, a remarkable visualization tool, played a pivotal role in crafting captivating presentations.\nThe following three charts encapsulate the efforts made during the Capstone project.\nPresentation.1 - Performance of various ML models for Loan default classification The following table shows the preformance score for each model. F1 Score is measure of models accuracy\nPresentation.2 - Selecting Top three classification Models for predicting loan default Every model has a Train and Test score. We would like to see these scores as close as possible. Presentation.3 - Selecting the best classification model for loan default Here both Stacking Classification and HistGradientBoosting Classifier have simillar or closer F1 Scores. Digging deeper we can see that HistGradientBoosting is better at predicting actual defaults. And that\u0026rsquo;s the winner\nðŸ”— Jupyter Notebook ðŸ”— HTML version ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/mit_capstone/","summary":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.","title":"MIT-GreatLearning Applied Data Science program Capstone Project"},{"content":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all\nSource : Katrin Weitek Richard van der Blom\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/linkedin-visualization/","summary":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all","title":"Prepare yourself for LinkedIn"},{"content":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/azurelandscape/","summary":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation","title":"Azure Landscape"},{"content":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/posts/first/","summary":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","title":"Scratchpad"},{"content":"Sreenivas Angara Resume\n","permalink":"https://cooolbabu.github.io/SreenivasAngara_beta1/experience/resume/","summary":"Sreenivas Angara Resume","title":"Sreenivas Angara Resume"}]
[{"content":"Platform Scale for a Post-Pandemic world by Sangeet Paul Choudry says\nPlatform manifesto - The Invisible Hand is the new Iron Fist\n\u0026ldquo;In a networked age, we are moving from a world of command and control to a self-serve world where participation is encouraged through an invisible hand powered by data, API and algorithms.\u0026rdquo; - Chapter 1.4, page 45\nAdam Smith\u0026rsquo;s concept of the Invisible Hand, outlined in his renowned work \u0026ldquo;The Wealth of Nations\u0026rdquo; in 1776, proposes that individuals pursuing their own self-interest inadvertently contribute to the greater good of society. In a free-market economy, Smith contended that rational self-interested individuals make choices that benefit not only themselves but also society as a whole. This unregulated interplay of supply and demand, steered by the Invisible Hand, fosters competition, innovation, and economic growth.\nNow, you might be wondering how this connects to something as seemingly unrelated as Artificial Intelligence (AI) and Machine Learning (ML) algorithms. Well, it\u0026rsquo;s not as ominous as it might sound.\nI once had the opportunity to work on a client project where every customer call was recorded and transformed into text. This wasn\u0026rsquo;t for any nefarious purposes; rather, the company\u0026rsquo;s aim was to enhance customer service by using past interactions to train their customer service representatives effectively.\nThis experience prompted me to consider the broader implications of using text from voice recordings to conduct \u0026lsquo;Sentiment Analysis\u0026rsquo; and evaluate customer behavior. By scoring customer behavior, we can gather qualitative data on various fronts:\nIn the retail sector, we can categorize customers as loyal, valuable, or annoying.\nIn the credit card industry, we can assess whether a customer disputing charges is genuinely in the right or just trying to get off the hook.\nIn appointment-based businesses, tracking customer reliability can help manage the cost of no-shows. May be the business can do a no-show on the customer.\nIn the realm of job interviews, a bold idea emerges: instead of solely relying on human judgment, HR could employ AI algorithms to rate interviewee responses as confident, knowledgeable, or halucinating.\nTraditionally, businesses have leaned on quantitative metrics to enhance their productivity and make informed decisions. Sentiment Analysis, however, offers a fresh approach. We\u0026rsquo;re venturing into a new world where we can evaluate people based on what they say, opening up a world of possibilities and challenges.\nSo, the next time you hear the familiar phrase, \u0026ldquo;This call is being recorded for training purposes,\u0026rdquo; consider who they\u0026rsquo;re training. It\u0026rsquo;s likely an AI algorithm that\u0026rsquo;s quietly profiling you. That exclusive TripleXtanium card you hold today could be stripped away based on your interactions by an AI algorithm. So, be kind and be nice.\nAdam Smith\u0026rsquo;s concept of the Invisible Hand continues to promote competition, innovation, and economic growth. However, it can also act as an Iron Fist, driving improvements in productivity and bolstering bottom-line revenue.\nThats \u0026ldquo;The Invisible Hand is the new Iron Fist\u0026rdquo; in a nut shell.\nPlatform Scale for a Post-Pandemic World on Amazon\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/platformscale/","summary":"Platform Scale for a Post-Pandemic world by Sangeet Paul Choudry says\nPlatform manifesto - The Invisible Hand is the new Iron Fist\n\u0026ldquo;In a networked age, we are moving from a world of command and control to a self-serve world where participation is encouraged through an invisible hand powered by data, API and algorithms.\u0026rdquo; - Chapter 1.4, page 45\nAdam Smith\u0026rsquo;s concept of the Invisible Hand, outlined in his renowned work \u0026ldquo;The Wealth of Nations\u0026rdquo; in 1776, proposes that individuals pursuing their own self-interest inadvertently contribute to the greater good of society.","title":"The Invisible Hand is the new Iron Fist"},{"content":"MLflow is an open-source platform designed to streamline the machine learning (ML) lifecycle. It was developed by Databricks and has gained widespread adoption within the data science and machine learning communities. MLflow provides a comprehensive set of tools and libraries for managing the end-to-end process of developing, deploying, and maintaining machine learning models.\nKey features of MLflow include:\nExperiment Tracking: MLflow allows data scientists and engineers to keep track of their experiments. It records parameters, metrics, and artifacts for each run, making it easy to compare different models and configurations.\nModel Packaging: With MLflow, you can package your models in a consistent format, making it straightforward to share models across teams or deploy them to production environments.\nModel Versioning: MLflow offers model versioning, which helps keep track of different iterations of a model, ensuring reproducibility and easy rollback to previous versions.\nModel Registry: The model registry in MLflow facilitates collaboration among data scientists by providing a centralized repository for managing, organizing, and auditing models.\nAutoML and Hyperparameter Tuning: MLflow includes tools for hyperparameter tuning and automatic machine learning (AutoML) to help optimize model performance.\nDeployment and Serving: You can deploy MLflow models to a variety of deployment targets, including cloud platforms and containerized environments, making it easier to put models into production.\nIntegration with Other Tools: MLflow can be integrated with popular libraries and tools like scikit-learn, TensorFlow, PyTorch, and more, allowing you to leverage your existing ML ecosystem.\nOverall, MLflow simplifies the complexities of the machine learning workflow, from experimentation and development to deployment and management. It promotes best practices for tracking experiments, collaborating on models, and ensuring model reproducibility, making it a valuable tool for both individual data scientists and teams working on machine learning projects.\nHere is sample notebook. Look for \u0026lsquo;## Prediction\u0026rsquo; for model predictions. To run this notebook, I had MLFlow running on my laptop. I am not going to say, that the setup was easy. It isn\u0026rsquo;t.\nPerseverance wins at the end of the day.\nClick here to view this notebook in full screen ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/mlflow101/","summary":"MLflow is an open-source platform designed to streamline the machine learning (ML) lifecycle. It was developed by Databricks and has gained widespread adoption within the data science and machine learning communities. MLflow provides a comprehensive set of tools and libraries for managing the end-to-end process of developing, deploying, and maintaining machine learning models.\nKey features of MLflow include:\nExperiment Tracking: MLflow allows data scientists and engineers to keep track of their experiments.","title":"MLFlow 101 - Dev to Production Environment in a single notebook"},{"content":"Think of Azure Data Factory\u0026rsquo;s Copy Activity as the Swiss Army knife of data integration. Just as a Swiss Army knife combines various tools into one compact design, Copy Activity seamlessly combines multiple data integration functions into a single, powerful feature.\nVersatility: Just like a Swiss Army knife\u0026rsquo;s ability to handle various tasks, Copy Activity connects to diverse data sources and destinations, whether they\u0026rsquo;re in the cloud or on-premises.\nEfficiency: Like a Swiss Army knife\u0026rsquo;s efficiency in handling different tasks, Copy Activity streamlines data movement and transformation in a single step. It lets you define data transformations during the copy process itself, eliminating the need for separate steps.\nOptimized Performance: Similar to a Swiss Army knife\u0026rsquo;s precision engineering, Copy Activity uses parallel processing and smart data transfer techniques to ensure your data moves quickly and securely.\nAdaptability: Much like how a Swiss Army knife can adapt to different situations, Copy Activity supports incremental loading, ensuring that only new or changed data is transferred, saving time and resources.\nIntegration: Just as a Swiss Army knife can be part of a multi-tool set, Copy Activity seamlessly integrates with other Azure services like Synapse Analytics and Databricks, allowing you to create comprehensive data workflows.\nUse Cases: Data Warehousing: Move data from different sources into data warehouses for analysis, just like a Swiss Army knife\u0026rsquo;s versatility in various situations.\nCloud Migration: Effortlessly transition data from on-premises systems to the cloud, similar to how a Swiss Army knife adapts to different tasks.\nReal-time Insights: Create real-time data pipelines for up-to-the-minute reporting, mirroring the adaptability of a Swiss Army knife.\nIn the dynamic world of data integration, Azure Data Factory\u0026rsquo;s Copy Activity shines as a true Swiss Army knife, simplifying data movement and transformation with its multi-faceted capabilities. Like the iconic tool, Copy Activity empowers efficient workflows, driving better insights and innovation.\nIllustrating a Streamlined Data Extraction: Retrieving \u0026ldquo;Call Logs\u0026rdquo; from Ring Central Let\u0026rsquo;s explore an uncomplicated yet effective implementation for extracting \u0026ldquo;Call Logs\u0026rdquo; from Ring Central. This process unfolds in two seamless steps, offering a practical example of the efficiency of Azure Data Factory\u0026rsquo;s Copy Activity.\nStep 1: Access Token Acquisition\nInitiate the process by employing your Ring Central credentials to obtain an \u0026lsquo;Access Token.\u0026rsquo; This token functions as a secure authentication key, granting you authorized access to Ring Central\u0026rsquo;s API.\nStep 2: Retrieving Call Logs with Access Token\nLeveraging the obtained Access Token, you can initiate the retrieval of desired \u0026ldquo;Call Logs\u0026rdquo; from Ring Central. This is effortlessly executed through Azure Data Factory\u0026rsquo;s Copy Activity. Configure the source as Ring Central\u0026rsquo;s API endpoint, and designate your preferred Azure storage or database as the destination.\nThis two-step procedure elegantly showcases the practicality and power of Azure Data Factory\u0026rsquo;s Copy Activity in streamlining data extraction. Through these steps, you can seamlessly access valuable insights from \u0026ldquo;Call Logs,\u0026rdquo; embodying the true spirit of efficient data integration.\nPipeline setup in Azure Data Factory Credentials are stored Azure Key Vault Pipeline parameters for Continous Delivery: Dev -\u0026gt; Staging -\u0026gt; Prod Github link to sourcecode\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/azurecopyactivity/","summary":"Think of Azure Data Factory\u0026rsquo;s Copy Activity as the Swiss Army knife of data integration. Just as a Swiss Army knife combines various tools into one compact design, Copy Activity seamlessly combines multiple data integration functions into a single, powerful feature.\nVersatility: Just like a Swiss Army knife\u0026rsquo;s ability to handle various tasks, Copy Activity connects to diverse data sources and destinations, whether they\u0026rsquo;re in the cloud or on-premises.\nEfficiency: Like a Swiss Army knife\u0026rsquo;s efficiency in handling different tasks, Copy Activity streamlines data movement and transformation in a single step.","title":"Azure Data Factory Copy Activity: The Swiss Army Knife of Data Integration"},{"content":"In a world fueled by technology, the concept of Digital Twins has emerged as a groundbreaking bridge between the physical and digital realms. Imagine having a virtual counterpart for real-world objects, processes, or systems that mimics their behavior and characteristics in real time. This technology is not just about static models – it\u0026rsquo;s about creating dynamic, synchronized virtual entities that offer a wealth of benefits across industries.\nDigital Twins rely on a mix of technologies like IoT, data analytics, machine learning, and cloud computing. IoT devices gather real-world data, which is then used to construct a virtual model that reflects real-time changes. This synchronization empowers organizations to monitor, analyze, and predict scenarios, driving innovation and efficiency.\nIndustries are leveraging Digital Twins in transformative ways:\nManufacturing: Optimize production, predict equipment failures, and enhance efficiency.\nHealthcare: Personalized treatments, surgical simulations, and medical training.\nSmart Cities: Efficient urban planning, traffic management, and energy optimization.\nEnergy: Optimize power plants, monitor energy consumption, and ensure reliability.\nAerospace: Monitor performance, predict maintenance, and enhance safety.\nRetail: Analyze customer behavior, refine store layouts, and manage inventory.\nReal Estate: Plan energy-efficient buildings, monitor usage, and plan maintenance.\nWhile Digital Twins offer immense potential, challenges like security and complexity persist. Yet, as technology advances and implementation expertise grows, the impact of Digital Twins will undoubtedly expand.\nIn essence, Digital Twins are reshaping how we interact with reality. By creating dynamic virtual replicas of the physical world, industries can make informed decisions, innovate in unprecedented ways, and pave the way for a future of interconnected solutions. As this concept continues to evolve, it promises to transform industries, ushering in an era of data-driven insights and transformative possibilities.\nGithub : Sample code\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/azuredigitaltwins/","summary":"In a world fueled by technology, the concept of Digital Twins has emerged as a groundbreaking bridge between the physical and digital realms. Imagine having a virtual counterpart for real-world objects, processes, or systems that mimics their behavior and characteristics in real time. This technology is not just about static models – it\u0026rsquo;s about creating dynamic, synchronized virtual entities that offer a wealth of benefits across industries.\nDigital Twins rely on a mix of technologies like IoT, data analytics, machine learning, and cloud computing.","title":"Digital Twins: Bridging the Physical and Virtual Worlds"},{"content":"As an enterprise architect, using text-based diagrams can be a smarter approach for several reasons. Text-based diagrams offer superior portability and accessibility, as they are lightweight and can be shared as plain text files through various communication channels, reaching a broader audience without the need for specialized software. Furthermore, by being version-controlled using tools like Git, text-based diagrams facilitate seamless collaboration among team members, enabling efficient tracking of changes and supporting geographically dispersed teams. The absence of proprietary formats ensures long-term compatibility and mitigates issues related to software transitions. Additionally, integrating text-based diagrams within documentation enhances readability, provides context-rich understanding, and simplifies collaborative editing, enabling real-time updates. Overall, this method empowers architects to create clear and concise representations, while being programmatically generated from data sources, enhancing automation, consistency, and accessibility for all stakeholders.\nI found Mermaid to be good enough for small use cases. However, for more complex diagrams others like PlantUML is more suitable. However, Mermaid has the potential to become a strong contender as they rollout new features.\nKey benefit of using Mermaid - No server installation. Entire diagrams are rendered using JavaScript in the browser.\nAs a test case, I took a lesson from The Ultimate Azure API Management course and tried to capture a sequence diagram and configuration steps using Timeline Diagram\n%%{init: {'sequence': { 'mirrorActors': false, 'rightAngles': true, 'messageAlign': 'center', 'actorFontSize': 20, 'actorFontWeight': 900, 'noteFontSize': 18, 'noteFontWeight': 600, 'messageFontSize': 14}}}%% %%{init: {'theme': 'base', 'themeVariables': {'labelBoxBkgColor': 'lightgrey', 'labelBoxBorderColor': '#000000', 'actorBorder': '#D86613', 'actorBkg': '#ffffff', 'activationBorderColor': '#232F3E', 'activationBkgColor': '#D86613', 'noteBkgColor': 'rgba(255, 153, 0, .25)', 'noteBorderColor': '#232F3E'}}}%% sequenceDiagram title Sequence diagram to call API Service using Azure B2C Active directory participant User participant AzureAD as Azure Active Directory participant APIService as Azure API Services participant AzureFunctions as Azure Functions note over AzureAD: 1. Authenticate with AzureAD B2C Tenant User -\u003e\u003e+ AzureAD: Authenticate AzureAD -\u003e\u003e AzureAD: Execute signin-signup policy AzureAD -\u003e\u003e- User: Authenticated - Return UserObjectId note over APIService: 2. Authorization user with AzureAD B2C Tenant User -\u003e\u003e+ APIService: Request Authorization APIService -\u003e\u003e+ AzureAD: Authorize - UserObjectId/ClientId/Secret/Scope AzureAD --\u003e\u003e- APIService: jwt-token APIService --\u003e\u003e- User: jwt-token note over APIService: 3. Call REST Service User -\u003e\u003e+ APIService: Invoke Products Service with jwt token APIService -\u003e\u003e APIService: jwt-validate-policy APIService -\u003e\u003e+ AzureFunctions: Call function method AzureFunctions --\u003e\u003e- APIService: Product details response APIService -\u003e\u003e APIService: Apply transformation policy APIService --\u003e\u003e- User: Products Data %%{init: { 'logLevel': 'debug', 'theme': 'default' , 'themeVariables': { 'cScale0': '#ff0055', 'cScaleLabel0': '#000', 'cScale1': '#b3ffd9', 'cScale2': '#99bbff', 'cScale3': '#001155', 'cScaleLabel3': '#ffffff' } } }%% timeline title Configuration - Securing API using Azure AD B2C section Config - Azure AD B2C Register BackendApp : Goto App Registration : Register BackendApp : Goto Certificates and Secrets : Create a secret : Goto Expose API : Create App Id Uri and scope : Save ClientId, Secret \u0026 Scope Register PortalApp : Goto App Registration : Register PortalApp - using Developer portal for this exercise : Goto Certificates and Secrets : Create a secret : Save ClientId \u0026 Secret : Add Authentation URL after configuring API Management services Create User flow : Goto Policies : Create Signup-Signin user flow : Attributes - Select Return Claim - userObjectId section Config - Azure Function Create Azure Function: Create FunctionApp : Create new function : Authentication - Add Identity Provider : Select Microsoft as Provider : Provide BackendApp details (ClientId/Secret) : Issuer Url - Signup-Signin user flow URL : NOTE- Connected Azure function to BackendApp registered in Azure B2C section Config - API Management Azure API Management Configuration : Import Function app : Goto - Config OAuth 2.0 : Set- Auth endpoint url : Set- Token endpoint Url : Set- Default Scope to BackendApp : Set- ClientId and Secret to BackendApp values : Save - Authorization grant flow url Use this value to set Authentication flow in PortalApp See Auth Flow step under Register Portal App : Configure Identities - Add identity Provider : Configure OAuth2.0 for API service section Test using Developer Portal Azure Developer Portal: Register user account : Validate user creation in AzureAD B2C tenant : Test API Service Source code for Sequence diagram\n%%{init: {'sequence': { 'mirrorActors': false, 'rightAngles': true, 'messageAlign': 'center', 'actorFontSize': 20, 'actorFontWeight': 900, 'noteFontSize': 18, 'noteFontWeight': 600, 'messageFontSize': 14}}}%% %%{init: {'theme': 'base', 'themeVariables': {'labelBoxBkgColor': 'lightgrey', 'labelBoxBorderColor': '#000000', 'actorBorder': '#D86613', 'actorBkg': '#ffffff', 'activationBorderColor': '#232F3E', 'activationBkgColor': '#D86613', 'noteBkgColor': 'rgba(255, 153, 0, .25)', 'noteBorderColor': '#232F3E'}}}%% sequenceDiagram title Sequence diagram to call API Service using Azure B2C Active directory participant User participant AzureAD as Azure Active Directory participant APIService as Azure API Services participant AzureFunctions as Azure Functions note over AzureAD: 1. Authenticate with AzureAD B2C Tenant User -\u0026gt;\u0026gt;+ AzureAD: Authenticate AzureAD -\u0026gt;\u0026gt; AzureAD: Execute signin-signup policy AzureAD -\u0026gt;\u0026gt;- User: Authenticated - Return UserObjectId note over APIService: 2. Authorization user with AzureAD B2C Tenant User -\u0026gt;\u0026gt;+ APIService: Request Authorization APIService -\u0026gt;\u0026gt;+ AzureAD: Authorize - UserObjectId/ClientId/Secret/Scope AzureAD --\u0026gt;\u0026gt;- APIService: jwt-token APIService --\u0026gt;\u0026gt;- User: jwt-token note over APIService: 3. Call REST Service User -\u0026gt;\u0026gt;+ APIService: Invoke Products Service with jwt token APIService -\u0026gt;\u0026gt; APIService: jwt-validate-policy APIService -\u0026gt;\u0026gt;+ AzureFunctions: Call function method AzureFunctions --\u0026gt;\u0026gt;- APIService: Product details response APIService -\u0026gt;\u0026gt; APIService: Apply transformation policy APIService --\u0026gt;\u0026gt;- User: Products Data Source code for Timeline\n%%{init: { 'logLevel': 'debug', 'theme': 'default' , 'themeVariables': { 'cScale0': '#ff0055', 'cScaleLabel0': '#000', 'cScale1': '#b3ffd9', 'cScale2': '#99bbff', 'cScale3': '#001155', 'cScaleLabel3': '#ffffff' } } }%% timeline title Configuration - Securing API using Azure AD B2C section Config - Azure AD B2C Register BackendApp : Goto App Registration : Register BackendApp : Goto Certificates and Secrets : Create a secret : Goto Expose API : Create App Id Uri and scope : Save ClientId, Secret \u0026amp; Scope Register PortalApp : Goto App Registration : Register PortalApp - using Developer portal for this exercise : Goto Certificates and Secrets : Create a secret : Save ClientId \u0026amp; Secret : Add Authentation URL after configuring API Management services Create User flow : Goto Policies : Create Signup-Signin user flow : Attributes - Select Return Claim - userObjectId section Config - Azure Function Create Azure Function: Create FunctionApp : Create new function : Authentication - Add Identity Provider : Select Microsoft as Provider : Provide BackendApp details (ClientId/Secret) : Issuer Url - Signup-Signin user flow URL : NOTE- Connected Azure function to BackendApp registered in Azure B2C section Config - API Management Azure API Management Configuration : Import Function app : Goto - Config OAuth 2.0 : Set- Auth endpoint url : Set- Token endpoint Url : Set- Default Scope to BackendApp : Set- ClientId and Secret to BackendApp values : Save - Authorization grant flow url \u0026lt;br\u0026gt; Use this value to set Authentication flow in PortalApp \u0026lt;br\u0026gt; See Auth Flow step under Register Portal App : Configure Identities - Add identity Provider : Configure OAuth2.0 for API service section Test using Developer Portal Azure Developer Portal: Register user account : Validate user creation in AzureAD B2C tenant : Test API Service References :\nKevin Hakanson - Sequence Diagrams with Mermaid and Diagrams.net Mermaid Chart- Create complex, visual diagrams with text. A smarter way of creating diagrams Configure Hugo to render Mermaid Charts ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/secureazureapib2c/","summary":"As an enterprise architect, using text-based diagrams can be a smarter approach for several reasons. Text-based diagrams offer superior portability and accessibility, as they are lightweight and can be shared as plain text files through various communication channels, reaching a broader audience without the need for specialized software. Furthermore, by being version-controlled using tools like Git, text-based diagrams facilitate seamless collaboration among team members, enabling efficient tracking of changes and supporting geographically dispersed teams.","title":"Secure Azure APIs using Azure B2C Active Directory - Sequence Diagram using Mermaid.js"},{"content":"Non-linear regression is a powerful technique used in statistics and machine learning to model and analyze complex relationships between variables. Unlike linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression algorithms can capture more intricate and non-linear patterns. This makes them particularly useful when dealing with real-world phenomena where linear relationships are insufficient.\nFollowing Non-Linear regression models are covered in the attached Jypter notebook\nDecision Tree Regressor: A decision tree regressor is a non-linear regression algorithm that creates a hierarchical structure of binary decisions to predict the target variable. It splits the data based on feature values and predicts the average value of the target variable within each leaf node. Decision trees are easy to interpret but can suffer from overfitting. Bagging Regressor: The Bagging Regressor is an ensemble learning algorithm that combines multiple decision tree regressors. It trains each decision tree on a random subset of the training data, allowing them to learn different patterns. The final prediction is the average of the predictions made by each tree. Bagging helps reduce overfitting and improves model stability. Random Forest Regressor: Random Forest is another ensemble algorithm that builds a collection of decision trees. It introduces randomness by selecting a subset of features at each split and training each tree independently. Random Forests are effective in capturing complex non-linear relationships and provide robust predictions by aggregating the outputs of multiple trees. Ada Boost Regressor: Ada Boost (Adaptive Boosting) is an ensemble algorithm that combines weak learners into a strong learner. In the context of regression, weak learners are decision trees with limited depth. Ada Boost assigns higher weights to data points that were incorrectly predicted by previous models, allowing subsequent models to focus on those points. It iteratively improves predictions by adjusting the weights of the weak learners. Gradient Boosting Regressor: Gradient Boosting Regressor is another ensemble algorithm that combines weak learners, typically decision trees, in a sequential manner. It fits each subsequent tree to the residual errors of the previous tree. By minimizing the errors in a gradient descent-like manner, Gradient Boosting produces a strong predictive model. It is particularly effective in handling complex non-linear relationships.\nXG Boost Regressor: XG Boost (Extreme Gradient Boosting) is an optimized implementation of the Gradient Boosting algorithm. It uses a more efficient and accurate tree-building process, handling missing values, regularization techniques, and parallel processing. XG Boost has gained popularity due to its high performance and effectiveness in various regression problems. These regression models offer different trade-offs in terms of interpretability, accuracy, and computational complexity. Choosing the most suitable model depends on the specific problem and dataset characteristics. It is often beneficial to experiment and compare multiple models to find the best fit for the task at hand.\nPerformance Report The highlighted model is a tuned Random Forest Regressor using GridSearch. We can see that both R-Square and Adjusted R-Square are higher and error scores are lower than models that are not tuned. Machine learning models performance can be improved with Hyperparameters and are essential for optimizing a model\nFeature Importance We can extract important features and visualize them. Here\nThe most important features are Department_gynecology, Age_41_50, and Age_31_40, followed by Department_anesthesia, Department_radiotherapy, and Admission_Deposit. The rest of the variables have little or no influence on the length of stay in the hospital in this model. 🔗 Jupyter Notebook 🔗 HTML version ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/non-linearregression/","summary":"Non-linear regression is a powerful technique used in statistics and machine learning to model and analyze complex relationships between variables. Unlike linear regression, which assumes a linear relationship between the independent and dependent variables, non-linear regression algorithms can capture more intricate and non-linear patterns. This makes them particularly useful when dealing with real-world phenomena where linear relationships are insufficient.\nFollowing Non-Linear regression models are covered in the attached Jypter notebook","title":"Non-Linear Regression models"},{"content":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.\nTrue being - the Loan will default False - the loan will not default An essential aspect of being a successful Data Scientist lies in effectively presenting findings. Seaborn, a remarkable visualization tool, played a pivotal role in crafting captivating presentations.\nThe following three charts encapsulate the efforts made during the Capstone project.\nPresentation.1 - Performance of various ML models for Loan default classification The following table shows the preformance score for each model. F1 Score is measure of models accuracy\nPresentation.2 - Selecting Top three classification Models for predicting loan default Every model has a Train and Test score. We would like to see these scores as close as possible. Presentation.3 - Selecting the best classification model for loan default Here both Stacking Classification and HistGradientBoosting Classifier have simillar or closer F1 Scores. Digging deeper we can see that HistGradientBoosting is better at predicting actual defaults. And that\u0026rsquo;s the winner\n🔗 Jupyter Notebook 🔗 HTML version ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/mit_capstone/","summary":"During my participation in the MIT-GreatLearning Applied Data Science Program, I experienced an intensive curriculum that encompassed diverse subjects such as Statistics, Python programming, Machine Learning, Neural Networks, and Recommendation Systems.\nThe culmination of the program was the Capstone project, which aimed to predict Loan Default. This involved crucial steps like Data Preparation, Exploratory Data Analysis, constructing models, and generating a comprehensive performance report for the various models employed.\nThe final outcome is a binary classification.","title":"MIT-GreatLearning Applied Data Science program Capstone Project"},{"content":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.\nGenerative AI is a disruptive technology that evokes both awe and apprehension. It has the potential to revolutionize numerous professions, and as a Machine Learning engineer, I was curious to explore its capabilities. Currently, I am enrolled in an \u0026ldquo;Applied Data Science and Machine Learning\u0026rdquo; bootcamp, aiming to discover the impact of Generative AI firsthand. It took me approximately six months to acquire proficiency in Python programming and Machine Learning techniques. To challenge myself further, I sought to explore a topic completely unfamiliar to me, something that hadn\u0026rsquo;t been covered in the course and hadn\u0026rsquo;t been searched on Google yet. Through an NPR segment, I came across Sentiment Analysis.\nDuring my exploration of Generative AI, I put it to the test by asking ChatGPT just two questions. Astonishingly, within a mere two minutes, it provided me with an answer. This experience left me truly amazed.\nQuestion 1 You are a data scientist performing Sentiment analysis. I have a customer satisfaction problem. I want to know, why customers are dissatisfied with my products in the Computers category. You need to read comments and tell me if .. 1) Product is high priced 2) Issue with product quality 3) unhappy with delivery. What deep learning algorithm should I use to solve this problem\nFor the customer satisfaction problem in the Computers category, where you want to identify the reasons for customer dissatisfaction based on comments, sentiment analysis can be an effective approach. Specifically, you can use a deep learning algorithm called a Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells.\nLSTM-based RNNs are well-suited for analyzing sequential data, such as sentences or comments, because they can capture the dependencies and long-term relationships between words. This makes them effective for sentiment analysis tasks where the order of words and their context is important.\nQuestion 2 Let\u0026rsquo;s use Recurrent Neural Network (RNN) with Long Short-Term Memory (LSTM) cells. The data is in a file that\u0026rsquo;s in Parquet format. It has customer_id, product_id and comments. Give me python machine learning code that implement this alogorithm: 1) That finds the best parameters to use and then use the best parameters to build a model that I can use in my production environment\nCertainly! To implement the RNN with LSTM cells for sentiment analysis using customer comments, and to find the best parameters for your model, you can follow the code example provided below. This example assumes you have the necessary libraries such as pandas, scikit-learn, TensorFlow, and Keras installed.\nimport pandas as pd import numpy as np from sklearn.model_selection import train_test_split from sklearn.metrics import accuracy_score from sklearn.model_selection import GridSearchCV from tensorflow.keras.preprocessing.text import Tokenizer from tensorflow.keras.preprocessing.sequence import pad_sequences from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Embedding, LSTM, Dense # Load the data from the Parquet file df = pd.read_parquet(\u0026quot;data.parquet\u0026quot;) # Split the data into input features (comments) and target variable (reason for dissatisfaction) X = df[\u0026quot;comments\u0026quot;].values y = df[\u0026quot;dissatisfaction_reason\u0026quot;].values # Split the data into training and testing sets X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42) # Tokenize the input text and convert it to sequences tokenizer = Tokenizer() tokenizer.fit_on_texts(X_train) X_train_seq = tokenizer.texts_to_sequences(X_train) X_test_seq = tokenizer.texts_to_sequences(X_test) # Pad the sequences to ensure they have the same length max_sequence_length = max([len(seq) for seq in X_train_seq]) X_train_padded = pad_sequences(X_train_seq, maxlen=max_sequence_length) X_test_padded = pad_sequences(X_test_seq, maxlen=max_sequence_length) # Define the LSTM model model = Sequential() model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) model.add(LSTM(units=128, dropout=0.2, recurrent_dropout=0.2)) model.add(Dense(units=3, activation='softmax')) # Compile the model model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Define the parameter grid for grid search param_grid = { 'units': [64, 128, 256], 'dropout': [0.2, 0.3, 0.4], 'recurrent_dropout': [0.2, 0.3, 0.4] } # Perform grid search to find the best parameters grid_search = GridSearchCV(estimator=model, param_grid=param_grid, cv=3) grid_search.fit(X_train_padded, y_train) # Get the best parameters best_params = grid_search.best_params_ print(\u0026quot;Best Parameters:\u0026quot;, best_params) # Build the final model using the best parameters final_model = Sequential() final_model.add(Embedding(input_dim=len(tokenizer.word_index) + 1, output_dim=100, input_length=max_sequence_length)) final_model.add(LSTM(units=best_params['units'], dropout=best_params['dropout'], recurrent_dropout=best_params['recurrent_dropout'])) final_model.add(Dense(units=3, activation='softmax')) final_model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy']) # Train the final model using the best parameters final_model.fit(X_train_padded, y_train, epochs=10, batch_size=32) # Evaluate the final model on the test set y_pred = final_model.predict_classes(X_test_padded) accuracy = accuracy_score(y_test, y_pred) print(\u0026quot;Accuracy:\u0026quot;, accuracy) # Save the final model for use in the product environment final_model.save(\u0026quot;sentiment_analysis_model.h5\u0026quot;) Final Thoughts Truly amazing! Prior to ChatGPT, we used to rely on code snippets that we copied and customized for our specific implementation. However, now we have the ability to obtain pre-customized programs that are ready to run. The next step is to construct a pipeline and execute the code.\nTaking it a step further, software developers can piece together individual components like puzzles and create a comprehensive final solution.\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/second/","summary":"As a Technical Consultant, I have successfully implemented software solutions using various programming languages, frameworks, and patterns across different platforms throughout my career. The emergence of Google has significantly enhanced my productivity, and I have always admired the knowledgeable community at Stackflow. There, I found individuals who had encountered obstacles before me and others who possessed the solutions. And now, with the advent of Generative AI, a whole new level of innovation has arrived.","title":"Conversations with ChatGPT - Machine Learning"},{"content":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all\nSource : Katrin Weitek Richard van der Blom\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/linkedin-visualization/","summary":"Every professional should dedicate a reasonable amount of time each week to LinkedIn. This commitment serves as an investment in job security, and there are two main reasons for doing so.\nPersonal Branding Your company HR is always looking for \u0026lsquo;Great Candidates\u0026rsquo; to fill a certain post. Why not let them find you on LinkedIn\nProfessional Networking If you are furloughed, find your next opportunity quickly\nFinally,\nIn the future, your LinkedIn score may determine whether you get to keep the job or not This visualiztion says it all","title":"Prepare yourself for LinkedIn"},{"content":"During an interview process, interviewers generally ask and expect textbook definitions of basic terms. Interviewees, on the other hand must be prepared and answer them promptly. Lengthy and exploratory answers prevent interviewers from asking more probing questions.\nConfidence Interval A confidence interval is a range of values around the point estimate, constructed so that this range will contain the population parameter with a certain degree of confidence, expressed in percentage terms. Used for testing null hypothesis\nVariance To calculate the variance, we take the difference between each number in the dataset and the mean of the data, square this difference to make it positive (independent of sign), and finally divide the sum of the squares by the total number of values in the dataset.\nBias Bias is the amount by which a model’s predictions differ from the target values, on the training data, i.e., the training error\nBias-Variance Trade-off If we increase the bias of an overfit model, we are making the model simpler and capable of generalizing over the validation set. Its performance on the validation set will improve and, consequently, the variance will decrease. On the other hand, if we decrease the bias of an underfit model, we are making the model more complex and the model will fit the training data more closely and, consequently, the variance will increase. This phenomenon is called the Bias-Variance tradeoff.\nHomoscedasticity - If the residuals are symmetrically distributed across the regression line, then the data is said to be homoscedastic.\nHeteroscedasticity - If the residuals are not symmetrically distributed across the regression line, then the data is said to be heteroscedastic. In this case, the residuals can form a funnel shape or any other non-symmetrical shape\nQ_Q plot: The plot obtained is known as a quantile-quantile plot, or Q_Q plot, when the quantiles of two variables are placed against one another. With respect to the locations, this plot summarises whether or not the distributions of two variables are similar.\nPearsonr: A linear relationship between two variables\u0026rsquo; strength and direction is measured by the Pearson correlation coefficient. Values usually fall between -1 and 1, with 1 denoting a perfectly positive correlation and -1 denoting a perfectly inverse relationship.\nMulticolinearity - occurs when there is a dependency between independent variables\nVIF Variance inflation Factor. Review Celestial Object detection for details. 1 very less Multicollinerity \u0026lt; 5 Moderat VIF \u0026gt; 5 Extreme Multicollinearity Solutions Principle Component Analysis Drop some predictors Do some linear transformation e.g., add/subtract 2 predictors to create a new bespoke predictor Linear Regression Model performance R-squared: R-squared is a useful performance metric to understand how well the regression model has fitted over the training data. For example, an R-squared of 80% reveals that the model is able to capture 80% of the variation in the dependent variable. A higher R-squared value indicates a better fit for the model. Adjusted R-squared: The adjusted R-squared is a modified version of R-squared that takes into account the number of independent variables present in the model. The R-squared always increases when a new independent variable is added to the model, irrespective of whether that variable adds value to the model or not. Hence, R-squared might be misleading when we have multiple independent variables and cannot identify unnecessary variables included in the model. RMSE: RMSE stands for Root Mean Squared Error. It is calculated as the square root of the mean of the squared differences between the actual output and the predictions. The lower the RMSE the better the performance of the model. Mathematically, it can be given as follows: However, when a new variable is added, the adjusted R-squared increases if that variable adds value to the model, and decreases if it does not. Hence, adjusted R-squared is a better choice of metric than R-squared to evaluate the quality of a regression model with multiple independent variables, because adjusted R-squared only remains high when all those independent variables are required to predict the value of the dependent variable well; it decreases if there are any independent variables which don\u0026rsquo;t have a significant effect on the predicted variable. MAE: The absolute error is the absolute value of the difference between the forecasted value and the actual value. MAE tells us how big of an error we can expect from the forecast on average. MAPE: The mean absolute percentage error (MAPE) measures the accuracy of predictions as a percentage, and can be calculated as the average of absolute percentage error for all data points. The absolute percentage error is defined as predicted value minus actual values divided by actual values. It works best if there are no extreme values in the data and none of the actual values are 0. Classification model performance Accuracy - Overall Accuracy of the model. Precision - Use Precision to minimize False Negatives - Ideal for maketing campaigns Recall - Use Recall to minimize False Positives - Telling someone he/she has \u0026lsquo;Cancer\u0026rsquo; when not true F1-Score - Geometric mean of Precision and Recall. Overall Model performance Area under curve - Coverage of accurate predictions. ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/ml_key_terms/","summary":"During an interview process, interviewers generally ask and expect textbook definitions of basic terms. Interviewees, on the other hand must be prepared and answer them promptly. Lengthy and exploratory answers prevent interviewers from asking more probing questions.\nConfidence Interval A confidence interval is a range of values around the point estimate, constructed so that this range will contain the population parameter with a certain degree of confidence, expressed in percentage terms.","title":"Machnine Language Terms - Cheat sheet"},{"content":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/azurelandscape/","summary":"This powerpoint presentation of various tiers in Microsoft Azure Cloud implementation.\nWith the architecture in-place, I want to answer the question\n\u0026ldquo;How can you leverage Generative AI to build and maintain this infrastructure?\u0026rdquo; This is an embedded Microsoft Office presentation, powered by Office. Full PowerPoint presentation","title":"Azure Landscape"},{"content":"🔗 Credly ","permalink":"https://cooolbabu.github.io/SreenivasAngara/experience/resume/","summary":"🔗 Credly ","title":"Sreenivas Angara Resume"},{"content":"Feels good to complete this program. Very excited about Machine Learning and Generative AI\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/experience/mitprogram/","summary":"Feels good to complete this program. Very excited about Machine Learning and Generative AI","title":"MIT Professional education - Applied Data Science and Machine Learning"},{"content":"WIP - Putting my thoughts together a future article on Azure API Management. Policies Authentication Policies Access Restriction Policy Caching Policy Cross Domain Policies Transformation Policies Advanced Policies\nMicroservice Microservices are fine grained loosely coupled services communicating through light-weight protocols. Azure functions versus Docker. Portability versus serverless\nAPI Management API Mangagement is a process of creating and publishing web application programming interfaces. Enforce usage policies, control access and analyse usage statistics\n","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/api_management/","summary":"WIP - Putting my thoughts together a future article on Azure API Management. Policies Authentication Policies Access Restriction Policy Caching Policy Cross Domain Policies Transformation Policies Advanced Policies\nMicroservice Microservices are fine grained loosely coupled services communicating through light-weight protocols. Azure functions versus Docker. Portability versus serverless\nAPI Management API Mangagement is a process of creating and publishing web application programming interfaces. Enforce usage policies, control access and analyse usage statistics","title":"WIP - API Management Concepts"},{"content":"WIP - A future article on Enterprise Application Platforms Platforms: JBoss, Microsoft Entity Framework Core, SpringBoot\nApplications: Web apps, REST services, SOA, JMS messaging\nLayers: Presentation layer, Buiness Layer, Database Layer (use mermaid.js for text to Illustration)\nKey concepts: Dependency Injection: Promotes loose coupling .. examples ??? Aspect Orient Programming: Apply application wide services to an application in a declarative fashion Logging, Transactions, Security POJO - Plain Old java object\nData Access Layer\nORM, JPA, Hibernate, JMS Build tools:\nMaven \u0026ndash; Personal shopper to get all your jar files \u0026ndash; Standard directory structure \u0026ndash; pom.xml ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/appcontainers/","summary":"WIP - A future article on Enterprise Application Platforms Platforms: JBoss, Microsoft Entity Framework Core, SpringBoot\nApplications: Web apps, REST services, SOA, JMS messaging\nLayers: Presentation layer, Buiness Layer, Database Layer (use mermaid.js for text to Illustration)\nKey concepts: Dependency Injection: Promotes loose coupling .. examples ??? Aspect Orient Programming: Apply application wide services to an application in a declarative fashion Logging, Transactions, Security POJO - Plain Old java object","title":"WIP - Application Platforms"},{"content":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","permalink":"https://cooolbabu.github.io/SreenivasAngara/posts/first/","summary":"This is a scratch post to view various features of PaperMod theme\nPowerpoint Presentation PowerPoint\nSome issues that still need to be worked upon This is a sample. Post with cover image. Need to work on this issue.\nThe following link doesn\u0026rsquo;t work.\nXlabel\nReport TitlePerformance xx Report\nUsing Raw html Write this text right-to-left\nUsing rtl shortcode Hello .. this is left to right message ","title":"Scratchpad"}]